{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76503568",
   "metadata": {},
   "source": [
    "# ğŸ¯ XGBoost Model Training Pipeline\n",
    "## State Business Risk Scoring Platform - Layer 1: ML Predictions\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ“‹ Notebook Overview\n",
    "\n",
    "This notebook trains **two production-ready XGBoost models** for predicting small business financial risk:\n",
    "\n",
    "| Model | Target | Type | Purpose |\n",
    "|-------|--------|------|--------|\n",
    "| **Classifier** | `default_flag` | Binary (0/1) | Will this business default in 12 months? |\n",
    "| **Regressor** | `risk_score` | Continuous (0-100) | What is the severity of risk? |\n",
    "\n",
    "### ğŸ”¢ Dataset Summary\n",
    "- **15,000 businesses** from Louisiana parishes\n",
    "- **43 input features** (financial, operational, credit, IGS benchmarks)\n",
    "- **2 target variables** (risk_score, default_flag)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a35a59e",
   "metadata": {},
   "source": [
    "## ğŸ“¦ 1. Environment Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cab27866",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 30\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;66;03m# Classification metrics\u001b[39;00m\n\u001b[1;32m     22\u001b[0m     accuracy_score, precision_score, recall_score, f1_score,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m     mean_absolute_error, mean_squared_error, r2_score\n\u001b[1;32m     27\u001b[0m )\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# XGBoost\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgb\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mxgboost\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m XGBClassifier, XGBRegressor\n\u001b[1;32m     33\u001b[0m \u001b[38;5;66;03m# Model persistence\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                           IMPORTS & CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Core libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "\n",
    "# Sklearn - preprocessing & model selection\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import (\n",
    "    # Classification metrics\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    roc_auc_score, roc_curve, precision_recall_curve, average_precision_score,\n",
    "    confusion_matrix, classification_report,\n",
    "    # Regression metrics\n",
    "    mean_absolute_error, mean_squared_error, r2_score\n",
    ")\n",
    "\n",
    "# XGBoost\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBClassifier, XGBRegressor\n",
    "\n",
    "# Model persistence\n",
    "import pickle\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                           PLOTTING CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Custom color palette for risk visualization\n",
    "RISK_COLORS = {\n",
    "    'low': '#2ECC71',      # Green\n",
    "    'medium': '#F39C12',   # Orange  \n",
    "    'high': '#E74C3C',     # Red\n",
    "    'primary': '#3498DB',  # Blue\n",
    "    'secondary': '#9B59B6' # Purple\n",
    "}\n",
    "\n",
    "# Gradient colormap for risk scores\n",
    "risk_cmap = LinearSegmentedColormap.from_list(\n",
    "    'risk', ['#2ECC71', '#F39C12', '#E74C3C'], N=100\n",
    ")\n",
    "\n",
    "# Figure defaults\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['font.size'] = 11\n",
    "plt.rcParams['axes.titlesize'] = 14\n",
    "plt.rcParams['axes.labelsize'] = 12\n",
    "\n",
    "print(\"âœ… Libraries imported successfully!\")\n",
    "print(f\"ğŸ“… Training started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(f\"ğŸ”§ XGBoost version: {xgb.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32b8b3b",
   "metadata": {},
   "source": [
    "## ğŸ“ 2. Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22779bc",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../datasets/synthetic_xgboost_training_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-2554197656.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"â•\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../datasets/synthetic_xgboost_training_data.csv'"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                              LOAD DATASET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Use relative path from the notebooks folder\n",
    "DATA_PATH = '../datasets/synthetic_xgboost_training_data.csv'\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "\n",
    "print(\"â•\" * 70)\n",
    "print(\"                    ğŸ“ DATASET LOADED SUCCESSFULLY\")\n",
    "print(\"â•\" * 70)\n",
    "print(f\"\\nğŸ“ Shape: {df.shape[0]:,} rows Ã— {df.shape[1]} columns\")\n",
    "print(f\"ğŸ’¾ Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"ğŸ“‚ Loaded from: {DATA_PATH}\")\n",
    "print(\"\\n\" + \"â”€\" * 70)\n",
    "print(\"ğŸ“‹ Column Overview:\")\n",
    "print(\"â”€\" * 70)\n",
    "\n",
    "# Categorize columns\n",
    "print(f\"\\n   ğŸ†” ID Column: business_id\")\n",
    "print(f\"   ğŸ¯ Target Variables: risk_score, default_flag\")\n",
    "print(f\"   ğŸ“Š Feature Columns: {df.shape[1] - 3} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9694b488",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‹ First 5 Rows:\n",
      "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-258777280.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nğŸ“‹ First 5 Rows:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"â”€\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                           DATA STRUCTURE OVERVIEW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nğŸ“‹ First 5 Rows:\")\n",
    "print(\"â”€\" * 70)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2474ee96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                              DATA TYPES & INFO\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š Data Types Summary:\")\n",
    "print(\"â”€\" * 70)\n",
    "\n",
    "dtype_counts = df.dtypes.value_counts()\n",
    "for dtype, count in dtype_counts.items():\n",
    "    print(f\"   {dtype}: {count} columns\")\n",
    "r\n",
    "print(\"\\nğŸ“‹ Column Details:\")\n",
    "print(\"â”€\" * 70)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e503a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                           MISSING VALUES ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "missing_df = pd.DataFrame({\n",
    "    'Missing Count': missing,\n",
    "    'Missing %': missing_pct\n",
    "}).sort_values('Missing Count', ascending=False)\n",
    "\n",
    "# Filter to show only columns with missing values\n",
    "missing_cols = missing_df[missing_df['Missing Count'] > 0]\n",
    "\n",
    "print(\"\\nğŸ” Missing Values Analysis:\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "if len(missing_cols) > 0:\n",
    "    print(f\"\\nâš ï¸  {len(missing_cols)} columns have missing values:\\n\")\n",
    "    print(missing_cols.to_string())\n",
    "else:\n",
    "    print(\"\\nâœ… No missing values found in the dataset!\")\n",
    "\n",
    "# Visualize missing values if any\n",
    "if len(missing_cols) > 0:\n",
    "    fig, ax = plt.subplots(figsize=(12, max(4, len(missing_cols) * 0.4)))\n",
    "    colors = [RISK_COLORS['high'] if pct > 10 else RISK_COLORS['medium'] if pct > 5 else RISK_COLORS['low'] \n",
    "              for pct in missing_cols['Missing %']]\n",
    "    bars = ax.barh(missing_cols.index, missing_cols['Missing %'], color=colors, edgecolor='white')\n",
    "    ax.set_xlabel('Missing Percentage (%)')\n",
    "    ax.set_title('ğŸ“Š Missing Values by Column', fontsize=14, fontweight='bold')\n",
    "    ax.axvline(x=10, color='red', linestyle='--', alpha=0.5, label='10% threshold')\n",
    "    \n",
    "    # Add value labels\n",
    "    for bar, pct in zip(bars, missing_cols['Missing %']):\n",
    "        ax.text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                f'{pct:.1f}%', va='center', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80643a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                         STATISTICAL SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“ˆ Statistical Summary (Numerical Features):\")\n",
    "print(\"â•\" * 70)\n",
    "df.describe().T.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468b1096",
   "metadata": {},
   "source": [
    "## ğŸ¯ 3. Target Variable Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d8d16b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                        TARGET VARIABLES OVERVIEW\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ¯ TARGET VARIABLES ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Default Flag Distribution\n",
    "default_counts = df['default_flag'].value_counts()\n",
    "default_pct = df['default_flag'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"\\nğŸ“Š default_flag (Binary Classification Target):\")\n",
    "print(\"â”€\" * 50)\n",
    "print(f\"   Class 0 (No Default): {default_counts[0]:,} ({default_pct[0]:.1f}%)\")\n",
    "print(f\"   Class 1 (Default):    {default_counts[1]:,} ({default_pct[1]:.1f}%)\")\n",
    "print(f\"   Imbalance Ratio:      {default_counts[0]/default_counts[1]:.2f}:1\")\n",
    "\n",
    "# Risk Score Distribution\n",
    "print(\"\\nğŸ“Š risk_score (Regression Target):\")\n",
    "print(\"â”€\" * 50)\n",
    "print(f\"   Min:    {df['risk_score'].min()}\")\n",
    "print(f\"   Max:    {df['risk_score'].max()}\")\n",
    "print(f\"   Mean:   {df['risk_score'].mean():.2f}\")\n",
    "print(f\"   Median: {df['risk_score'].median()}\")\n",
    "print(f\"   Std:    {df['risk_score'].std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972c4b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    TARGET VARIABLES VISUALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Plot 1: Default Flag Distribution (Pie Chart)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "colors_pie = [RISK_COLORS['low'], RISK_COLORS['high']]\n",
    "explode = (0, 0.05)\n",
    "axes[0].pie(default_counts.values, labels=['No Default (0)', 'Default (1)'], \n",
    "            autopct='%1.1f%%', colors=colors_pie, explode=explode,\n",
    "            shadow=True, startangle=90, textprops={'fontsize': 11})\n",
    "axes[0].set_title('ğŸ¯ Default Flag Distribution\\n(Classification Target)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Plot 2: Risk Score Distribution (Histogram)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "n, bins, patches = axes[1].hist(df['risk_score'], bins=50, edgecolor='white', alpha=0.8)\n",
    "\n",
    "# Color bars by risk level\n",
    "for i, (patch, left_edge) in enumerate(zip(patches, bins[:-1])):\n",
    "    if left_edge < 40:\n",
    "        patch.set_facecolor(RISK_COLORS['low'])\n",
    "    elif left_edge < 75:\n",
    "        patch.set_facecolor(RISK_COLORS['medium'])\n",
    "    else:\n",
    "        patch.set_facecolor(RISK_COLORS['high'])\n",
    "\n",
    "axes[1].axvline(df['risk_score'].mean(), color='black', linestyle='--', \n",
    "                linewidth=2, label=f\"Mean: {df['risk_score'].mean():.1f}\")\n",
    "axes[1].axvline(df['risk_score'].median(), color='blue', linestyle=':', \n",
    "                linewidth=2, label=f\"Median: {df['risk_score'].median()}\")\n",
    "axes[1].set_xlabel('Risk Score')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('ğŸ“Š Risk Score Distribution\\n(Regression Target)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].legend(loc='upper right')\n",
    "\n",
    "# Add risk band annotations\n",
    "axes[1].axvspan(0, 40, alpha=0.1, color=RISK_COLORS['low'], label='Low Risk')\n",
    "axes[1].axvspan(40, 75, alpha=0.1, color=RISK_COLORS['medium'], label='Medium Risk')\n",
    "axes[1].axvspan(75, 100, alpha=0.1, color=RISK_COLORS['high'], label='High Risk')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Plot 3: Risk Score by Default Flag (Box Plot)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "bp = axes[2].boxplot([df[df['default_flag']==0]['risk_score'], \n",
    "                      df[df['default_flag']==1]['risk_score']],\n",
    "                     labels=['No Default (0)', 'Default (1)'],\n",
    "                     patch_artist=True)\n",
    "\n",
    "bp['boxes'][0].set_facecolor(RISK_COLORS['low'])\n",
    "bp['boxes'][1].set_facecolor(RISK_COLORS['high'])\n",
    "for box in bp['boxes']:\n",
    "    box.set_alpha(0.7)\n",
    "\n",
    "axes[2].set_ylabel('Risk Score')\n",
    "axes[2].set_title('ğŸ”— Risk Score by Default Status\\n(Target Correlation)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate correlation between targets\n",
    "corr = df['risk_score'].corr(df['default_flag'])\n",
    "print(f\"\\nğŸ“ˆ Correlation between risk_score and default_flag: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522f80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                         RISK BANDS ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Create risk bands\n",
    "df['risk_band'] = pd.cut(df['risk_score'], \n",
    "                          bins=[-1, 40, 75, 100], \n",
    "                          labels=['Low (0-40)', 'Medium (41-75)', 'High (76-100)'])\n",
    "\n",
    "risk_band_stats = df.groupby('risk_band').agg({\n",
    "    'risk_score': ['count', 'mean', 'std'],\n",
    "    'default_flag': 'mean'\n",
    "}).round(3)\n",
    "\n",
    "risk_band_stats.columns = ['Count', 'Avg Risk Score', 'Std Dev', 'Default Rate']\n",
    "risk_band_stats['Percentage'] = (risk_band_stats['Count'] / len(df) * 100).round(1)\n",
    "\n",
    "print(\"\\nğŸ“Š Risk Band Distribution:\")\n",
    "print(\"â•\" * 70)\n",
    "print(risk_band_stats.to_string())\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Risk band counts\n",
    "colors = [RISK_COLORS['low'], RISK_COLORS['medium'], RISK_COLORS['high']]\n",
    "bars = axes[0].bar(risk_band_stats.index, risk_band_stats['Count'], \n",
    "                   color=colors, edgecolor='white', linewidth=2)\n",
    "axes[0].set_ylabel('Number of Businesses')\n",
    "axes[0].set_title('ğŸ“Š Businesses by Risk Band', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, risk_band_stats['Count']):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 100, \n",
    "                 f'{count:,}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Default rate by risk band\n",
    "bars2 = axes[1].bar(risk_band_stats.index, risk_band_stats['Default Rate'] * 100, \n",
    "                    color=colors, edgecolor='white', linewidth=2)\n",
    "axes[1].set_ylabel('Default Rate (%)')\n",
    "axes[1].set_title('ğŸ“ˆ Default Rate by Risk Band', fontsize=13, fontweight='bold')\n",
    "axes[1].set_ylim(0, 100)\n",
    "\n",
    "# Add percentage labels\n",
    "for bar, rate in zip(bars2, risk_band_stats['Default Rate']):\n",
    "    axes[1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2, \n",
    "                 f'{rate*100:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Clean up temporary column\n",
    "df.drop('risk_band', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977e388c",
   "metadata": {},
   "source": [
    "## ğŸ” 4. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fb49261",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    INDUSTRY & GEOGRAPHIC DISTRIBUTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Industry Sector Distribution with Default Rate\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "industry_stats = df.groupby('industry_sector').agg({\n",
    "    'business_id': 'count',\n",
    "    'default_flag': 'mean',\n",
    "    'risk_score': 'mean'\n",
    "}).rename(columns={'business_id': 'count', 'default_flag': 'default_rate', 'risk_score': 'avg_risk'})\n",
    "industry_stats = industry_stats.sort_values('count', ascending=True)\n",
    "\n",
    "colors_ind = plt.cm.viridis(np.linspace(0.2, 0.8, len(industry_stats)))\n",
    "bars = axes[0].barh(industry_stats.index, industry_stats['count'], color=colors_ind, edgecolor='white')\n",
    "axes[0].set_xlabel('Number of Businesses')\n",
    "axes[0].set_title('ğŸ­ Industry Sector Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars, industry_stats['count']):\n",
    "    axes[0].text(bar.get_width() + 50, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{count:,}', va='center', fontsize=9)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Parish/Region Distribution with Default Rate\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "parish_stats = df.groupby('region_parish').agg({\n",
    "    'business_id': 'count',\n",
    "    'default_flag': 'mean',\n",
    "    'risk_score': 'mean'\n",
    "}).rename(columns={'business_id': 'count', 'default_flag': 'default_rate', 'risk_score': 'avg_risk'})\n",
    "parish_stats = parish_stats.sort_values('count', ascending=True)\n",
    "\n",
    "colors_par = plt.cm.plasma(np.linspace(0.2, 0.8, len(parish_stats)))\n",
    "bars2 = axes[1].barh(parish_stats.index, parish_stats['count'], color=colors_par, edgecolor='white')\n",
    "axes[1].set_xlabel('Number of Businesses')\n",
    "axes[1].set_title('ğŸ—ºï¸ Parish/Region Distribution', fontsize=13, fontweight='bold')\n",
    "\n",
    "# Add count labels\n",
    "for bar, count in zip(bars2, parish_stats['count']):\n",
    "    axes[1].text(bar.get_width() + 30, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{count:,}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9267cd19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                     DEFAULT RATE BY CATEGORY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Default Rate by Industry (sorted)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "industry_default = df.groupby('industry_sector')['default_flag'].mean().sort_values(ascending=True) * 100\n",
    "\n",
    "colors_def = [RISK_COLORS['low'] if x < 20 else RISK_COLORS['medium'] if x < 30 else RISK_COLORS['high'] \n",
    "              for x in industry_default.values]\n",
    "bars = axes[0].barh(industry_default.index, industry_default.values, color=colors_def, edgecolor='white')\n",
    "axes[0].axvline(x=df['default_flag'].mean()*100, color='black', linestyle='--', linewidth=2, \n",
    "                label=f\"Overall Avg: {df['default_flag'].mean()*100:.1f}%\")\n",
    "axes[0].set_xlabel('Default Rate (%)')\n",
    "axes[0].set_title('âš ï¸ Default Rate by Industry', fontsize=13, fontweight='bold')\n",
    "axes[0].legend(loc='lower right')\n",
    "\n",
    "for bar, rate in zip(bars, industry_default.values):\n",
    "    axes[0].text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{rate:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Default Rate by Parish (sorted)\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "parish_default = df.groupby('region_parish')['default_flag'].mean().sort_values(ascending=True) * 100\n",
    "\n",
    "colors_par = [RISK_COLORS['low'] if x < 20 else RISK_COLORS['medium'] if x < 30 else RISK_COLORS['high'] \n",
    "              for x in parish_default.values]\n",
    "bars2 = axes[1].barh(parish_default.index, parish_default.values, color=colors_par, edgecolor='white')\n",
    "axes[1].axvline(x=df['default_flag'].mean()*100, color='black', linestyle='--', linewidth=2,\n",
    "                label=f\"Overall Avg: {df['default_flag'].mean()*100:.1f}%\")\n",
    "axes[1].set_xlabel('Default Rate (%)')\n",
    "axes[1].set_title('âš ï¸ Default Rate by Parish', fontsize=13, fontweight='bold')\n",
    "axes[1].legend(loc='lower right')\n",
    "\n",
    "for bar, rate in zip(bars2, parish_default.values):\n",
    "    axes[1].text(bar.get_width() + 0.5, bar.get_y() + bar.get_height()/2, \n",
    "                 f'{rate:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa7470b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    KEY FINANCIAL METRICS DISTRIBUTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Key financial features to visualize\n",
    "financial_features = [\n",
    "    'annual_revenue', 'revenue_growth_yoy', 'profit_margin', \n",
    "    'avg_bank_balance', 'debt_to_revenue', 'cash_to_monthly_expense_ratio'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(financial_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Split by default status\n",
    "    data_0 = df[df['default_flag'] == 0][feature].dropna()\n",
    "    data_1 = df[df['default_flag'] == 1][feature].dropna()\n",
    "    \n",
    "    # Plot overlapping histograms\n",
    "    ax.hist(data_0, bins=50, alpha=0.6, color=RISK_COLORS['low'], \n",
    "            label='No Default', density=True, edgecolor='white')\n",
    "    ax.hist(data_1, bins=50, alpha=0.6, color=RISK_COLORS['high'], \n",
    "            label='Default', density=True, edgecolor='white')\n",
    "    \n",
    "    ax.set_xlabel(feature.replace('_', ' ').title())\n",
    "    ax.set_ylabel('Density')\n",
    "    ax.set_title(f'ğŸ’° {feature.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n",
    "    ax.legend(loc='upper right')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ğŸ“Š Financial Features Distribution by Default Status', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a519b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    CREDIT & RISK METRICS DISTRIBUTION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Credit-related features\n",
    "credit_features = [\n",
    "    'credit_score_business', 'days_past_due_max_last_12m', 'num_returned_payments_last_12m',\n",
    "    'missed_payroll_count_last_12m', 'overdraft_days_last_12m', 'utilization_rate_card'\n",
    "]\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(credit_features):\n",
    "    ax = axes[idx]\n",
    "    \n",
    "    # Box plot by default status\n",
    "    data = [df[df['default_flag']==0][feature].dropna(), \n",
    "            df[df['default_flag']==1][feature].dropna()]\n",
    "    \n",
    "    bp = ax.boxplot(data, labels=['No Default', 'Default'], patch_artist=True)\n",
    "    bp['boxes'][0].set_facecolor(RISK_COLORS['low'])\n",
    "    bp['boxes'][1].set_facecolor(RISK_COLORS['high'])\n",
    "    for box in bp['boxes']:\n",
    "        box.set_alpha(0.7)\n",
    "    \n",
    "    ax.set_ylabel(feature.replace('_', ' ').title())\n",
    "    ax.set_title(f'ğŸ“‹ {feature.replace(\"_\", \" \").title()}', fontsize=11, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ğŸ“Š Credit & Risk Features by Default Status', \n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aeb31ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                         CORRELATION ANALYSIS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Select numerical columns for correlation\n",
    "numerical_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "numerical_cols = [c for c in numerical_cols if c != 'business_id']\n",
    "\n",
    "# Calculate correlations with targets\n",
    "corr_with_default = df[numerical_cols].corr()['default_flag'].drop(['default_flag', 'risk_score']).sort_values()\n",
    "corr_with_risk = df[numerical_cols].corr()['risk_score'].drop(['default_flag', 'risk_score']).sort_values()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 10))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Correlation with default_flag\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "colors_corr = [RISK_COLORS['high'] if x > 0 else RISK_COLORS['low'] for x in corr_with_default.values]\n",
    "axes[0].barh(corr_with_default.index, corr_with_default.values, color=colors_corr, edgecolor='white')\n",
    "axes[0].axvline(x=0, color='black', linewidth=1)\n",
    "axes[0].set_xlabel('Correlation Coefficient')\n",
    "axes[0].set_title('ğŸ”— Feature Correlation with default_flag', fontsize=13, fontweight='bold')\n",
    "axes[0].set_xlim(-0.5, 0.5)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Correlation with risk_score\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "colors_corr2 = [RISK_COLORS['high'] if x > 0 else RISK_COLORS['low'] for x in corr_with_risk.values]\n",
    "axes[1].barh(corr_with_risk.index, corr_with_risk.values, color=colors_corr2, edgecolor='white')\n",
    "axes[1].axvline(x=0, color='black', linewidth=1)\n",
    "axes[1].set_xlabel('Correlation Coefficient')\n",
    "axes[1].set_title('ğŸ”— Feature Correlation with risk_score', fontsize=13, fontweight='bold')\n",
    "axes[1].set_xlim(-0.5, 0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top correlations\n",
    "print(\"\\nğŸ“Š Top 10 Features POSITIVELY Correlated with Default:\")\n",
    "print(\"â”€\" * 50)\n",
    "for feat, corr in corr_with_default.tail(10)[::-1].items():\n",
    "    print(f\"   {feat}: {corr:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ“Š Top 10 Features NEGATIVELY Correlated with Default:\")\n",
    "print(\"â”€\" * 50)\n",
    "for feat, corr in corr_with_default.head(10).items():\n",
    "    print(f\"   {feat}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b118fd5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    CORRELATION HEATMAP (Top Features)\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Select top correlated features for heatmap\n",
    "top_features = list(corr_with_default.abs().sort_values(ascending=False).head(15).index)\n",
    "top_features = top_features + ['risk_score', 'default_flag']\n",
    "\n",
    "corr_matrix = df[top_features].corr()\n",
    "\n",
    "plt.figure(figsize=(14, 12))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, fmt='.2f', \n",
    "            cmap='RdBu_r', center=0, square=True,\n",
    "            linewidths=0.5, cbar_kws={'shrink': 0.8})\n",
    "\n",
    "plt.title('ğŸ”¥ Correlation Heatmap: Top Features + Targets', \n",
    "          fontsize=14, fontweight='bold', pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c870317",
   "metadata": {},
   "source": [
    "## âš™ï¸ 5. Data Preprocessing & Feature Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1c0280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                      DEFINE FEATURE CATEGORIES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ› ï¸ FEATURE ENGINEERING & PREPROCESSING\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Define columns\n",
    "ID_COL = 'business_id'\n",
    "TARGET_COLS = ['risk_score', 'default_flag']\n",
    "CATEGORICAL_COLS = ['industry_sector', 'region_parish']\n",
    "\n",
    "# All feature columns (excluding ID and targets)\n",
    "FEATURE_COLS = [col for col in df.columns if col not in [ID_COL] + TARGET_COLS]\n",
    "\n",
    "# Numerical features (excluding categorical)\n",
    "NUMERICAL_COLS = [col for col in FEATURE_COLS if col not in CATEGORICAL_COLS]\n",
    "\n",
    "print(f\"\\nğŸ“Š Column Summary:\")\n",
    "print(f\"   â€¢ ID Column: {ID_COL}\")\n",
    "print(f\"   â€¢ Target Columns: {TARGET_COLS}\")\n",
    "print(f\"   â€¢ Categorical Features: {len(CATEGORICAL_COLS)}\")\n",
    "print(f\"   â€¢ Numerical Features: {len(NUMERICAL_COLS)}\")\n",
    "print(f\"   â€¢ Total Features: {len(FEATURE_COLS)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22c957a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                      HANDLE MISSING VALUES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”§ Handling Missing Values...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Create a copy for processing\n",
    "df_processed = df.copy()\n",
    "\n",
    "# Check missing values before\n",
    "missing_before = df_processed[NUMERICAL_COLS].isnull().sum().sum()\n",
    "print(f\"\\n   Missing values BEFORE: {missing_before:,}\")\n",
    "\n",
    "# Strategy: Fill numerical missing values with median (robust to outliers)\n",
    "for col in NUMERICAL_COLS:\n",
    "    if df_processed[col].isnull().any():\n",
    "        median_val = df_processed[col].median()\n",
    "        df_processed[col].fillna(median_val, inplace=True)\n",
    "        print(f\"   âœ” {col}: filled with median ({median_val:.4f})\")\n",
    "\n",
    "# Check missing values after\n",
    "missing_after = df_processed[NUMERICAL_COLS].isnull().sum().sum()\n",
    "print(f\"\\n   Missing values AFTER: {missing_after}\")\n",
    "print(\"\\nâœ… Missing value handling complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f47c561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                      ENCODE CATEGORICAL FEATURES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ·ï¸ Encoding Categorical Features...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Store label encoders for later use\n",
    "label_encoders = {}\n",
    "\n",
    "for col in CATEGORICAL_COLS:\n",
    "    le = LabelEncoder()\n",
    "    df_processed[col + '_encoded'] = le.fit_transform(df_processed[col].astype(str))\n",
    "    label_encoders[col] = le\n",
    "    \n",
    "    n_unique = len(le.classes_)\n",
    "    print(f\"\\n   ğŸ·ï¸ {col}:\")\n",
    "    print(f\"      Unique values: {n_unique}\")\n",
    "    print(f\"      Classes: {list(le.classes_)}\")\n",
    "\n",
    "# Update feature columns to use encoded versions\n",
    "FEATURE_COLS_FINAL = NUMERICAL_COLS + [col + '_encoded' for col in CATEGORICAL_COLS]\n",
    "\n",
    "print(f\"\\n\\nâœ… Final feature count: {len(FEATURE_COLS_FINAL)}\")\n",
    "print(f\"\\nğŸ“ Final Feature List:\")\n",
    "for i, col in enumerate(FEATURE_COLS_FINAL, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa19d71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                      PREPARE TRAIN/TEST SPLIT\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ”€ Preparing Train/Test Split...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Define random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "# Features matrix\n",
    "X = df_processed[FEATURE_COLS_FINAL].values\n",
    "feature_names = FEATURE_COLS_FINAL\n",
    "\n",
    "# Target for Classification\n",
    "y_class = df_processed['default_flag'].values\n",
    "\n",
    "# Target for Regression\n",
    "y_reg = df_processed['risk_score'].values\n",
    "\n",
    "# Stratified split for classification (maintains class distribution)\n",
    "X_train, X_test, y_train_class, y_test_class, y_train_reg, y_test_reg = train_test_split(\n",
    "    X, y_class, y_reg,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y_class  # Stratify by default_flag to maintain class balance\n",
    ")\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Dataset Split:\")\n",
    "print(f\"      Training set: {len(X_train):,} samples ({(1-TEST_SIZE)*100:.0f}%)\")\n",
    "print(f\"      Test set:     {len(X_test):,} samples ({TEST_SIZE*100:.0f}%)\")\n",
    "print(f\"\\n   ğŸ¯ Classification Target (default_flag):\")\n",
    "print(f\"      Train - Class 0: {sum(y_train_class==0):,} | Class 1: {sum(y_train_class==1):,}\")\n",
    "print(f\"      Test  - Class 0: {sum(y_test_class==0):,} | Class 1: {sum(y_test_class==1):,}\")\n",
    "print(f\"\\n   ğŸ“Š Regression Target (risk_score):\")\n",
    "print(f\"      Train - Mean: {y_train_reg.mean():.2f} | Std: {y_train_reg.std():.2f}\")\n",
    "print(f\"      Test  - Mean: {y_test_reg.mean():.2f} | Std: {y_test_reg.std():.2f}\")\n",
    "\n",
    "print(\"\\nâœ… Data preparation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a6ae9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ¤– 6. XGBoost Classifier Training (default_flag)\n",
    "\n",
    "Training a binary classifier to predict whether a business will default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137ff094",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    XGBOOST CLASSIFIER CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ¤– XGBOOST CLASSIFIER TRAINING\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Calculate scale_pos_weight for imbalanced classes\n",
    "scale_pos_weight = sum(y_train_class == 0) / sum(y_train_class == 1)\n",
    "print(f\"\\nâš–ï¸ Class imbalance ratio: {scale_pos_weight:.2f}:1\")\n",
    "print(f\"   Using scale_pos_weight={scale_pos_weight:.2f} to balance classes\")\n",
    "\n",
    "# Define classifier hyperparameters\n",
    "CLASSIFIER_PARAMS = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'scale_pos_weight': scale_pos_weight,\n",
    "    'objective': 'binary:logistic',\n",
    "    'eval_metric': 'auc',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Classifier Hyperparameters:\")\n",
    "print(\"â”€\" * 50)\n",
    "for param, value in CLASSIFIER_PARAMS.items():\n",
    "    print(f\"   {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631e7b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    TRAIN CLASSIFIER WITH CROSS-VALIDATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ‹ï¸ Training XGBoost Classifier...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Initialize classifier\n",
    "xgb_classifier = XGBClassifier(**CLASSIFIER_PARAMS)\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n   ğŸ”„ Performing 5-Fold Stratified Cross-Validation...\")\n",
    "cv_classifier = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_scores_roc = cross_val_score(xgb_classifier, X_train, y_train_class, \n",
    "                                 cv=cv_classifier, scoring='roc_auc', n_jobs=-1)\n",
    "cv_scores_f1 = cross_val_score(xgb_classifier, X_train, y_train_class, \n",
    "                                cv=cv_classifier, scoring='f1', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Cross-Validation Results:\")\n",
    "print(f\"      ROC-AUC: {cv_scores_roc.mean():.4f} (+/- {cv_scores_roc.std()*2:.4f})\")\n",
    "print(f\"      F1 Score: {cv_scores_f1.mean():.4f} (+/- {cv_scores_f1.std()*2:.4f})\")\n",
    "\n",
    "# Train final model on full training set\n",
    "print(\"\\n   ğŸš€ Training final model on full training set...\")\n",
    "xgb_classifier.fit(X_train, y_train_class, \n",
    "                   eval_set=[(X_train, y_train_class), (X_test, y_test_class)],\n",
    "                   verbose=False)\n",
    "\n",
    "print(\"\\nâœ… Classifier training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e21f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    CLASSIFIER EVALUATION ON TEST SET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š CLASSIFIER EVALUATION (Test Set)\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_class = xgb_classifier.predict(X_test)\n",
    "y_pred_proba = xgb_classifier.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(y_test_class, y_pred_class)\n",
    "precision = precision_score(y_test_class, y_pred_class)\n",
    "recall = recall_score(y_test_class, y_pred_class)\n",
    "f1 = f1_score(y_test_class, y_pred_class)\n",
    "roc_auc = roc_auc_score(y_test_class, y_pred_proba)\n",
    "avg_precision = average_precision_score(y_test_class, y_pred_proba)\n",
    "\n",
    "print(\"\\n   ğŸ† Test Set Performance:\")\n",
    "print(\"   â”€\" * 40)\n",
    "print(f\"      Accuracy:          {accuracy:.4f}\")\n",
    "print(f\"      Precision:         {precision:.4f}\")\n",
    "print(f\"      Recall:            {recall:.4f}\")\n",
    "print(f\"      F1 Score:          {f1:.4f}\")\n",
    "print(f\"      ROC-AUC:           {roc_auc:.4f}\")\n",
    "print(f\"      Avg Precision:     {avg_precision:.4f}\")\n",
    "\n",
    "# Classification Report\n",
    "print(\"\\n   ğŸ“‹ Classification Report:\")\n",
    "print(\"   â”€\" * 40)\n",
    "print(classification_report(y_test_class, y_pred_class, target_names=['No Default', 'Default']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3513f25d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    CLASSIFIER VISUALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Confusion Matrix\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "cm = confusion_matrix(y_test_class, y_pred_class)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0, 0],\n",
    "            xticklabels=['No Default', 'Default'], \n",
    "            yticklabels=['No Default', 'Default'])\n",
    "axes[0, 0].set_xlabel('Predicted')\n",
    "axes[0, 0].set_ylabel('Actual')\n",
    "axes[0, 0].set_title('ğŸ¯ Confusion Matrix', fontsize=13, fontweight='bold')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. ROC Curve\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "fpr, tpr, thresholds = roc_curve(y_test_class, y_pred_proba)\n",
    "axes[0, 1].plot(fpr, tpr, color=RISK_COLORS['primary'], linewidth=2, \n",
    "                label=f'ROC Curve (AUC = {roc_auc:.4f})')\n",
    "axes[0, 1].plot([0, 1], [0, 1], 'k--', linewidth=1, label='Random Classifier')\n",
    "axes[0, 1].fill_between(fpr, tpr, alpha=0.2, color=RISK_COLORS['primary'])\n",
    "axes[0, 1].set_xlabel('False Positive Rate')\n",
    "axes[0, 1].set_ylabel('True Positive Rate')\n",
    "axes[0, 1].set_title('ğŸ“ˆ ROC Curve', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend(loc='lower right')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Precision-Recall Curve\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "precision_curve, recall_curve, _ = precision_recall_curve(y_test_class, y_pred_proba)\n",
    "axes[1, 0].plot(recall_curve, precision_curve, color=RISK_COLORS['secondary'], linewidth=2,\n",
    "                label=f'PR Curve (AP = {avg_precision:.4f})')\n",
    "axes[1, 0].fill_between(recall_curve, precision_curve, alpha=0.2, color=RISK_COLORS['secondary'])\n",
    "axes[1, 0].set_xlabel('Recall')\n",
    "axes[1, 0].set_ylabel('Precision')\n",
    "axes[1, 0].set_title('ğŸ“‰ Precision-Recall Curve', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].legend(loc='lower left')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Prediction Probability Distribution\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "axes[1, 1].hist(y_pred_proba[y_test_class==0], bins=50, alpha=0.6, \n",
    "                color=RISK_COLORS['low'], label='Actual: No Default', density=True)\n",
    "axes[1, 1].hist(y_pred_proba[y_test_class==1], bins=50, alpha=0.6, \n",
    "                color=RISK_COLORS['high'], label='Actual: Default', density=True)\n",
    "axes[1, 1].axvline(x=0.5, color='black', linestyle='--', linewidth=2, label='Threshold (0.5)')\n",
    "axes[1, 1].set_xlabel('Predicted Probability of Default')\n",
    "axes[1, 1].set_ylabel('Density')\n",
    "axes[1, 1].set_title('ğŸ“Š Prediction Probability Distribution', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('ğŸ¤– XGBoost Classifier - Model Evaluation', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daa5b247",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“Š 7. XGBoost Regressor Training (risk_score)\n",
    "\n",
    "Training a regressor to predict the continuous risk score (0-100)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9132d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    XGBOOST REGRESSOR CONFIGURATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ“Š XGBOOST REGRESSOR TRAINING\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Define regressor hyperparameters\n",
    "REGRESSOR_PARAMS = {\n",
    "    'n_estimators': 300,\n",
    "    'max_depth': 6,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 3,\n",
    "    'gamma': 0.1,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'objective': 'reg:squarederror',\n",
    "    'eval_metric': 'rmse',\n",
    "    'random_state': RANDOM_STATE,\n",
    "    'n_jobs': -1,\n",
    "    'verbosity': 0\n",
    "}\n",
    "\n",
    "print(\"\\nğŸ“‹ Regressor Hyperparameters:\")\n",
    "print(\"â”€\" * 50)\n",
    "for param, value in REGRESSOR_PARAMS.items():\n",
    "    print(f\"   {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f8c161a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    TRAIN REGRESSOR WITH CROSS-VALIDATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ‹ï¸ Training XGBoost Regressor...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Initialize regressor\n",
    "xgb_regressor = XGBRegressor(**REGRESSOR_PARAMS)\n",
    "\n",
    "# Cross-validation\n",
    "print(\"\\n   ğŸ”„ Performing 5-Fold Cross-Validation...\")\n",
    "cv_regressor = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "cv_scores_r2 = cross_val_score(xgb_regressor, X_train, y_train_reg, \n",
    "                                cv=cv_regressor, scoring='r2', n_jobs=-1)\n",
    "cv_scores_mae = cross_val_score(xgb_regressor, X_train, y_train_reg, \n",
    "                                 cv=cv_regressor, scoring='neg_mean_absolute_error', n_jobs=-1)\n",
    "\n",
    "print(f\"\\n   ğŸ“Š Cross-Validation Results:\")\n",
    "print(f\"      RÂ² Score: {cv_scores_r2.mean():.4f} (+/- {cv_scores_r2.std()*2:.4f})\")\n",
    "print(f\"      MAE: {-cv_scores_mae.mean():.4f} (+/- {cv_scores_mae.std()*2:.4f})\")\n",
    "\n",
    "# Train final model on full training set\n",
    "print(\"\\n   ğŸš€ Training final model on full training set...\")\n",
    "xgb_regressor.fit(X_train, y_train_reg,\n",
    "                  eval_set=[(X_train, y_train_reg), (X_test, y_test_reg)],\n",
    "                  verbose=False)\n",
    "\n",
    "print(\"\\nâœ… Regressor training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db5579f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    REGRESSOR EVALUATION ON TEST SET\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ“Š REGRESSOR EVALUATION (Test Set)\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Get predictions\n",
    "y_pred_reg = xgb_regressor.predict(X_test)\n",
    "\n",
    "# Clip predictions to valid range [0, 100]\n",
    "y_pred_reg_clipped = np.clip(y_pred_reg, 0, 100)\n",
    "\n",
    "# Calculate metrics\n",
    "mae = mean_absolute_error(y_test_reg, y_pred_reg_clipped)\n",
    "mse = mean_squared_error(y_test_reg, y_pred_reg_clipped)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test_reg, y_pred_reg_clipped)\n",
    "\n",
    "print(\"\\n   ğŸ† Test Set Performance:\")\n",
    "print(\"   â”€\" * 40)\n",
    "print(f\"      MAE (Mean Absolute Error):     {mae:.4f}\")\n",
    "print(f\"      MSE (Mean Squared Error):      {mse:.4f}\")\n",
    "print(f\"      RMSE (Root Mean Squared Error): {rmse:.4f}\")\n",
    "print(f\"      RÂ² Score:                       {r2:.4f}\")\n",
    "\n",
    "# Additional insights\n",
    "print(\"\\n   ğŸ“Š Prediction Statistics:\")\n",
    "print(\"   â”€\" * 40)\n",
    "print(f\"      Actual    - Mean: {y_test_reg.mean():.2f} | Std: {y_test_reg.std():.2f}\")\n",
    "print(f\"      Predicted - Mean: {y_pred_reg_clipped.mean():.2f} | Std: {y_pred_reg_clipped.std():.2f}\")\n",
    "print(f\"      Correlation: {np.corrcoef(y_test_reg, y_pred_reg_clipped)[0,1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5c4175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    REGRESSOR VISUALIZATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 1. Actual vs Predicted Scatter Plot\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "scatter = axes[0, 0].scatter(y_test_reg, y_pred_reg_clipped, \n",
    "                              c=y_test_reg, cmap=risk_cmap, alpha=0.5, s=20)\n",
    "axes[0, 0].plot([0, 100], [0, 100], 'k--', linewidth=2, label='Perfect Prediction')\n",
    "axes[0, 0].set_xlabel('Actual Risk Score')\n",
    "axes[0, 0].set_ylabel('Predicted Risk Score')\n",
    "axes[0, 0].set_title(f'ğŸ¯ Actual vs Predicted (RÂ² = {r2:.4f})', fontsize=13, fontweight='bold')\n",
    "axes[0, 0].legend(loc='upper left')\n",
    "axes[0, 0].set_xlim(-5, 105)\n",
    "axes[0, 0].set_ylim(-5, 105)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "plt.colorbar(scatter, ax=axes[0, 0], label='Actual Risk Score')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 2. Residual Distribution\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "residuals = y_test_reg - y_pred_reg_clipped\n",
    "axes[0, 1].hist(residuals, bins=50, color=RISK_COLORS['primary'], \n",
    "                edgecolor='white', alpha=0.7)\n",
    "axes[0, 1].axvline(x=0, color='red', linestyle='--', linewidth=2, label='Zero Error')\n",
    "axes[0, 1].axvline(x=residuals.mean(), color='black', linestyle='-', linewidth=2, \n",
    "                   label=f'Mean: {residuals.mean():.2f}')\n",
    "axes[0, 1].set_xlabel('Residual (Actual - Predicted)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('ğŸ“Š Residual Distribution', fontsize=13, fontweight='bold')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 3. Residuals vs Predicted\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "axes[1, 0].scatter(y_pred_reg_clipped, residuals, alpha=0.4, s=15, c=RISK_COLORS['secondary'])\n",
    "axes[1, 0].axhline(y=0, color='red', linestyle='--', linewidth=2)\n",
    "axes[1, 0].set_xlabel('Predicted Risk Score')\n",
    "axes[1, 0].set_ylabel('Residual')\n",
    "axes[1, 0].set_title('ğŸ“‰ Residuals vs Predicted', fontsize=13, fontweight='bold')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# 4. Error by Risk Band\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "risk_bands_test = pd.cut(y_test_reg, bins=[-1, 40, 75, 100], \n",
    "                          labels=['Low (0-40)', 'Medium (41-75)', 'High (76-100)'])\n",
    "\n",
    "resid_by_band = pd.DataFrame({'residual': np.abs(residuals), 'band': risk_bands_test})\n",
    "band_errors = resid_by_band.groupby('band')['residual'].agg(['mean', 'std'])\n",
    "\n",
    "colors_band = [RISK_COLORS['low'], RISK_COLORS['medium'], RISK_COLORS['high']]\n",
    "bars = axes[1, 1].bar(band_errors.index, band_errors['mean'], \n",
    "                       yerr=band_errors['std'], color=colors_band, \n",
    "                       edgecolor='white', capsize=5)\n",
    "axes[1, 1].set_xlabel('Risk Band')\n",
    "axes[1, 1].set_ylabel('Mean Absolute Error')\n",
    "axes[1, 1].set_title('ğŸ¯ Prediction Error by Risk Band', fontsize=13, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "for bar, mae_val in zip(bars, band_errors['mean']):\n",
    "    axes[1, 1].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                    f'{mae_val:.2f}', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.suptitle('ğŸ“Š XGBoost Regressor - Model Evaluation', fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "368676f3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸŒŸ 8. Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b4f372",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    FEATURE IMPORTANCE COMPARISON\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸŒŸ FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Get feature importances from both models\n",
    "classifier_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': xgb_classifier.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "regressor_importance = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': xgb_regressor.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Top 15 features for each model\n",
    "top_n = 15\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Classifier Feature Importance\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "top_class = classifier_importance.head(top_n).iloc[::-1]\n",
    "colors = plt.cm.Blues(np.linspace(0.4, 0.9, top_n))\n",
    "axes[0].barh(top_class['feature'], top_class['importance'], color=colors, edgecolor='white')\n",
    "axes[0].set_xlabel('Feature Importance (Gain)')\n",
    "axes[0].set_title('ğŸ¤– Classifier: Top 15 Features\\n(default_flag prediction)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[0].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# Regressor Feature Importance\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "top_reg = regressor_importance.head(top_n).iloc[::-1]\n",
    "colors = plt.cm.Oranges(np.linspace(0.4, 0.9, top_n))\n",
    "axes[1].barh(top_reg['feature'], top_reg['importance'], color=colors, edgecolor='white')\n",
    "axes[1].set_xlabel('Feature Importance (Gain)')\n",
    "axes[1].set_title('ğŸ“Š Regressor: Top 15 Features\\n(risk_score prediction)', \n",
    "                   fontsize=13, fontweight='bold')\n",
    "axes[1].grid(True, alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print top features\n",
    "print(\"\\nğŸ† Top 10 Features for Classifier (default_flag):\")\n",
    "print(\"â”€\" * 50)\n",
    "for i, row in classifier_importance.head(10).iterrows():\n",
    "    print(f\"   {classifier_importance.head(10).index.tolist().index(i)+1:2d}. {row['feature']}: {row['importance']:.4f}\")\n",
    "\n",
    "print(\"\\nğŸ† Top 10 Features for Regressor (risk_score):\")\n",
    "print(\"â”€\" * 50)\n",
    "for i, row in regressor_importance.head(10).iterrows():\n",
    "    print(f\"   {regressor_importance.head(10).index.tolist().index(i)+1:2d}. {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff8b03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    FEATURE IMPORTANCE CORRELATION\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# Merge importances to compare\n",
    "importance_comparison = classifier_importance.merge(\n",
    "    regressor_importance, on='feature', suffixes=('_classifier', '_regressor')\n",
    ")\n",
    "\n",
    "# Calculate correlation between importances\n",
    "importance_corr = importance_comparison['importance_classifier'].corr(\n",
    "    importance_comparison['importance_regressor']\n",
    ")\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(importance_comparison['importance_classifier'], \n",
    "            importance_comparison['importance_regressor'],\n",
    "            c=range(len(importance_comparison)), cmap='viridis', s=100, alpha=0.7)\n",
    "\n",
    "# Add feature labels for top features\n",
    "for _, row in importance_comparison.nlargest(10, 'importance_classifier').iterrows():\n",
    "    plt.annotate(row['feature'].replace('_', '\\n'), \n",
    "                 (row['importance_classifier'], row['importance_regressor']),\n",
    "                 fontsize=8, alpha=0.8)\n",
    "\n",
    "plt.plot([0, importance_comparison['importance_classifier'].max()], \n",
    "         [0, importance_comparison['importance_classifier'].max()], \n",
    "         'k--', alpha=0.5, label='Equal Importance')\n",
    "\n",
    "plt.xlabel('Classifier Importance', fontsize=12)\n",
    "plt.ylabel('Regressor Importance', fontsize=12)\n",
    "plt.title(f'ğŸ”— Feature Importance Correlation\\n(r = {importance_corr:.4f})', \n",
    "          fontsize=14, fontweight='bold')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nğŸ“Š Feature importance correlation between models: {importance_corr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2473a86f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ’¾ 9. Save Models & Artifacts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03d49ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    CREATE OUTPUT DIRECTORY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"ğŸ’¾ SAVING MODELS & ARTIFACTS\")\n",
    "print(\"â•\" * 70)\n",
    "\n",
    "# Create models directory (relative path from notebooks folder)\n",
    "MODEL_DIR = '../models'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "print(f\"\\nğŸ“ Output directory: {MODEL_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a99912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    SAVE MODELS AS PICKLE FILES\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸš€ Saving trained models...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Save Classifier\n",
    "classifier_path = os.path.join(MODEL_DIR, 'xgboost_classifier.pkl')\n",
    "with open(classifier_path, 'wb') as f:\n",
    "    pickle.dump(xgb_classifier, f)\n",
    "print(f\"   âœ” Classifier saved: {classifier_path}\")\n",
    "\n",
    "# Save Regressor\n",
    "regressor_path = os.path.join(MODEL_DIR, 'xgboost_regressor.pkl')\n",
    "with open(regressor_path, 'wb') as f:\n",
    "    pickle.dump(xgb_regressor, f)\n",
    "print(f\"   âœ” Regressor saved: {regressor_path}\")\n",
    "\n",
    "# Save Feature Names\n",
    "feature_names_path = os.path.join(MODEL_DIR, 'feature_names.pkl')\n",
    "with open(feature_names_path, 'wb') as f:\n",
    "    pickle.dump(feature_names, f)\n",
    "print(f\"   âœ” Feature names saved: {feature_names_path}\")\n",
    "\n",
    "# Save Label Encoders\n",
    "encoders_path = os.path.join(MODEL_DIR, 'label_encoders.pkl')\n",
    "with open(encoders_path, 'wb') as f:\n",
    "    pickle.dump(label_encoders, f)\n",
    "print(f\"   âœ” Label encoders saved: {encoders_path}\")\n",
    "\n",
    "print(\"\\nâœ… All models and artifacts saved successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54baa0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    SAVE MODEL METADATA\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "import json\n",
    "\n",
    "model_metadata = {\n",
    "    'training_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'dataset_size': len(df),\n",
    "    'train_size': len(X_train),\n",
    "    'test_size': len(X_test),\n",
    "    'n_features': len(feature_names),\n",
    "    'feature_names': feature_names,\n",
    "    'xgboost_version': xgb.__version__,\n",
    "    'classifier': {\n",
    "        'params': CLASSIFIER_PARAMS,\n",
    "        'cv_roc_auc_mean': float(cv_scores_roc.mean()),\n",
    "        'cv_roc_auc_std': float(cv_scores_roc.std()),\n",
    "        'test_roc_auc': float(roc_auc),\n",
    "        'test_f1': float(f1),\n",
    "        'test_precision': float(precision),\n",
    "        'test_recall': float(recall)\n",
    "    },\n",
    "    'regressor': {\n",
    "        'params': REGRESSOR_PARAMS,\n",
    "        'cv_r2_mean': float(cv_scores_r2.mean()),\n",
    "        'cv_r2_std': float(cv_scores_r2.std()),\n",
    "        'test_r2': float(r2),\n",
    "        'test_mae': float(mae),\n",
    "        'test_rmse': float(rmse)\n",
    "    }\n",
    "}\n",
    "\n",
    "metadata_path = os.path.join(MODEL_DIR, 'model_metadata.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(model_metadata, f, indent=2)\n",
    "print(f\"\\nğŸ“ Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc127071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    VERIFY SAVED MODELS\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\nğŸ” Verifying saved models...\")\n",
    "print(\"â”€\" * 50)\n",
    "\n",
    "# Load and verify classifier\n",
    "with open(classifier_path, 'rb') as f:\n",
    "    loaded_classifier = pickle.load(f)\n",
    "y_verify_class = loaded_classifier.predict(X_test[:5])\n",
    "print(f\"   âœ” Classifier loaded and verified\")\n",
    "print(f\"      Sample predictions: {y_verify_class}\")\n",
    "\n",
    "# Load and verify regressor\n",
    "with open(regressor_path, 'rb') as f:\n",
    "    loaded_regressor = pickle.load(f)\n",
    "y_verify_reg = loaded_regressor.predict(X_test[:5])\n",
    "print(f\"   âœ” Regressor loaded and verified\")\n",
    "print(f\"      Sample predictions: {np.round(y_verify_reg, 2)}\")\n",
    "\n",
    "print(\"\\nâœ… All models verified successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f2f5a3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“ 10. Training Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec3fc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    FINAL TRAINING SUMMARY\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"â•”\" + \"â•\" * 70 + \"â•—\")\n",
    "print(\"â•‘\" + \" \" * 20 + \"ğŸ† TRAINING COMPLETE! ğŸ†\" + \" \" * 20 + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 70 + \"â•£\")\n",
    "print(\"â•‘\" + \" \" * 70 + \"â•‘\")\n",
    "print(f\"â•‘  ğŸ“… Training Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S'):<51}â•‘\")\n",
    "print(f\"â•‘  ğŸ“Š Dataset Size: {len(df):,} businesses{' ' * 41}â•‘\")\n",
    "print(f\"â•‘  ğŸ¯ Features Used: {len(feature_names)} features{' ' * 42}â•‘\")\n",
    "print(\"â•‘\" + \" \" * 70 + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 70 + \"â•£\")\n",
    "print(\"â•‘\" + \" ğŸ¤– CLASSIFIER (default_flag)\" + \" \" * 42 + \"â•‘\")\n",
    "print(\"â•‘\" + \"â”€\" * 70 + \"â•‘\")\n",
    "print(f\"â•‘     Cross-Val ROC-AUC: {cv_scores_roc.mean():.4f} (Â±{cv_scores_roc.std()*2:.4f}){' ' * 29}â•‘\")\n",
    "print(f\"â•‘     Test ROC-AUC:      {roc_auc:.4f}{' ' * 42}â•‘\")\n",
    "print(f\"â•‘     Test F1 Score:     {f1:.4f}{' ' * 42}â•‘\")\n",
    "print(f\"â•‘     Test Precision:    {precision:.4f}{' ' * 42}â•‘\")\n",
    "print(f\"â•‘     Test Recall:       {recall:.4f}{' ' * 42}â•‘\")\n",
    "print(\"â•‘\" + \" \" * 70 + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 70 + \"â•£\")\n",
    "print(\"â•‘\" + \" ğŸ“Š REGRESSOR (risk_score)\" + \" \" * 45 + \"â•‘\")\n",
    "print(\"â•‘\" + \"â”€\" * 70 + \"â•‘\")\n",
    "print(f\"â•‘     Cross-Val RÂ²:     {cv_scores_r2.mean():.4f} (Â±{cv_scores_r2.std()*2:.4f}){' ' * 30}â•‘\")\n",
    "print(f\"â•‘     Test RÂ²:          {r2:.4f}{' ' * 42}â•‘\")\n",
    "print(f\"â•‘     Test MAE:         {mae:.4f}{' ' * 42}â•‘\")\n",
    "print(f\"â•‘     Test RMSE:        {rmse:.4f}{' ' * 42}â•‘\")\n",
    "print(\"â•‘\" + \" \" * 70 + \"â•‘\")\n",
    "print(\"â• \" + \"â•\" * 70 + \"â•£\")\n",
    "print(\"â•‘\" + \" ğŸ’¾ SAVED ARTIFACTS\" + \" \" * 51 + \"â•‘\")\n",
    "print(\"â•‘\" + \"â”€\" * 70 + \"â•‘\")\n",
    "print(f\"â•‘     â€¢ xgboost_classifier.pkl{' ' * 44}â•‘\")\n",
    "print(f\"â•‘     â€¢ xgboost_regressor.pkl{' ' * 45}â•‘\")\n",
    "print(f\"â•‘     â€¢ feature_names.pkl{' ' * 48}â•‘\")\n",
    "print(f\"â•‘     â€¢ label_encoders.pkl{' ' * 47}â•‘\")\n",
    "print(f\"â•‘     â€¢ model_metadata.json{' ' * 46}â•‘\")\n",
    "print(\"â•‘\" + \" \" * 70 + \"â•‘\")\n",
    "print(\"â•š\" + \"â•\" * 70 + \"â•\")\n",
    "print(\"\\nğŸš€ Models are ready for production deployment!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b63d45a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ğŸ“š Usage Example\n",
    "\n",
    "Here's how to load and use the trained models in production:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92d47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#                    PRODUCTION USAGE EXAMPLE\n",
    "# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "\"\"\"\n",
    "# How to load and use the models in production:\n",
    "\n",
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Load models\n",
    "with open('models/xgboost_classifier.pkl', 'rb') as f:\n",
    "    classifier = pickle.load(f)\n",
    "    \n",
    "with open('models/xgboost_regressor.pkl', 'rb') as f:\n",
    "    regressor = pickle.load(f)\n",
    "    \n",
    "with open('models/feature_names.pkl', 'rb') as f:\n",
    "    feature_names = pickle.load(f)\n",
    "    \n",
    "with open('models/label_encoders.pkl', 'rb') as f:\n",
    "    encoders = pickle.load(f)\n",
    "\n",
    "# Prepare new business data (43 features)\n",
    "new_business = np.array([...])  # Your feature values\n",
    "\n",
    "# Get predictions\n",
    "default_probability = classifier.predict_proba(new_business.reshape(1, -1))[0, 1]\n",
    "default_flag = classifier.predict(new_business.reshape(1, -1))[0]\n",
    "risk_score = np.clip(regressor.predict(new_business.reshape(1, -1))[0], 0, 100)\n",
    "\n",
    "# Determine risk band\n",
    "if risk_score < 40:\n",
    "    risk_band = \"Low\"\n",
    "elif risk_score < 75:\n",
    "    risk_band = \"Medium\"\n",
    "else:\n",
    "    risk_band = \"High\"\n",
    "\n",
    "print(f\"Default Probability: {default_probability:.2%}\")\n",
    "print(f\"Default Flag: {default_flag}\")\n",
    "print(f\"Risk Score: {risk_score:.1f}\")\n",
    "print(f\"Risk Band: {risk_band}\")\n",
    "\"\"\"\n",
    "\n",
    "print(\"âœ… Example code shown above (as docstring)\")\n",
    "print(\"\\nğŸ‰ Notebook execution complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_data_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
